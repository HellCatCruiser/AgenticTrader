{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a315296",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "- https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b3713",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a2250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from datetime import datetime, timedelta\n",
    "from ta.momentum import RSIIndicator, StochasticOscillator\n",
    "from ta.trend import SMAIndicator, MACD, CCIIndicator\n",
    "from ta.momentum import WilliamsRIndicator\n",
    "from ta.volume import ChaikinMoneyFlowIndicator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a640f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cpu\n",
      "None\n",
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda) \n",
    "print(torch.cuda.is_available())   # True if a GPU is detected\n",
    "print(torch.cuda.device_count())   # Number of GPUs available\n",
    "# print(torch.cuda.get_device_name(0))  # Name of the first GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb54f9",
   "metadata": {},
   "source": [
    "#### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b4a6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_price_data(\n",
    "    ticker: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    pred_target: int,\n",
    "    leadup_days: int = 30,\n",
    "    interval: str = \"1d\",\n",
    "    sma_vals=[10, 20]\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetches price data with extra leadup_days for indicator warmup,\n",
    "    computes technical indicators, and trims back to the exact start_date.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute leadup start date\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    leadup_start_dt = start_dt - timedelta(days=leadup_days)\n",
    "    leadup_start = leadup_start_dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Fetch data\n",
    "    ticker_obj = yf.Ticker(ticker)\n",
    "    df = ticker_obj.history(\n",
    "        start=leadup_start,\n",
    "        end=end_date,\n",
    "        interval=interval,\n",
    "        auto_adjust=False,\n",
    "        actions=False\n",
    "    )\n",
    "\n",
    "    # Ensure columns are consistent\n",
    "    df = df.copy()\n",
    "    df.index = df.index.normalize()\n",
    "    df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # === Add Indicators ===\n",
    "    # SMA\n",
    "    for i in sma_vals:\n",
    "        df[f\"sma_{i}\"] = SMAIndicator(df[\"close\"], window=i).sma_indicator()\n",
    "\n",
    "    # Stochastic Oscillator\n",
    "    stoch = StochasticOscillator(\n",
    "        df[\"high\"], df[\"low\"], df[\"close\"],\n",
    "        window=14, smooth_window=3\n",
    "    )\n",
    "    df[\"stoch_k\"] = stoch.stoch()\n",
    "    df[\"stoch_d\"] = stoch.stoch_signal()\n",
    "\n",
    "    # MACD\n",
    "    macd = MACD(df[\"close\"], window_slow=26, window_fast=12, window_sign=9)\n",
    "    df[\"macd\"] = macd.macd()\n",
    "    df[\"macd_signal\"] = macd.macd_signal()\n",
    "    df[\"macd_hist\"] = macd.macd_diff()\n",
    "\n",
    "    # CCI\n",
    "    df[\"cci\"] = CCIIndicator(df[\"high\"], df[\"low\"], df[\"close\"], window=20).cci()\n",
    "\n",
    "    # Williams %R\n",
    "    df[\"williams_r\"] = WilliamsRIndicator(df[\"high\"], df[\"low\"], df[\"close\"], lbp=14).williams_r()\n",
    "\n",
    "    # RSI\n",
    "    df[\"rsi\"] = RSIIndicator(df[\"close\"], window=14).rsi()\n",
    "\n",
    "    # ADOSC (Chaikin Money Flow)\n",
    "    df[\"adosc\"] = ChaikinMoneyFlowIndicator(\n",
    "        high=df[\"high\"],\n",
    "        low=df[\"low\"],\n",
    "        close=df[\"close\"],\n",
    "        volume=df[\"volume\"],\n",
    "        window=20\n",
    "    ).chaikin_money_flow()\n",
    "    \n",
    "    # Target value\n",
    "    df[f\"{pred_target}-day_target\"] = (df[\"close\"].shift(-pred_target) - df[\"close\"]) / df[\"close\"]\n",
    "\n",
    "    # Drop NaNs caused by indicator warmup\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Trim back to the actual start_date (remove leadup)\n",
    "    df = df[df.index >= start_date]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46627ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(df, target_col ,feature_cols, seq_len=10):\n",
    "    X, y = [], []\n",
    "    \n",
    "    values = df[feature_cols].values\n",
    "    targets = target_col.values\n",
    "\n",
    "    for i in range(len(df) - seq_len):\n",
    "        X.append(values[i:i+seq_len])\n",
    "        y.append(targets[i+seq_len])\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1495256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_metrics(metrics_df, target_horizon, save_table=True):\n",
    "      \"\"\"Creates and saves three metric plots:\n",
    "         (1) RMSE + MAE together\n",
    "         (2) Directional Accuracy\n",
    "         (3) Spearman Correlation\n",
    "         (4) Metrics table of all years\n",
    "         Files are named using the prediction horizon.\n",
    "      \"\"\"\n",
    "      \n",
    "      # Create directory name and ensure it exists\n",
    "      base_folder = \"results\"\n",
    "      folder_name = os.path.join(base_folder, f\"target_{target_horizon}\")\n",
    "      os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "      years = metrics_df[\"year\"]\n",
    "\n",
    "      # --- Plot 1: RMSE + MAE ---\n",
    "      fig1, ax1 = plt.subplots(figsize=(8, 4))\n",
    "      ax1.plot(years, metrics_df[\"rmse\"], marker=\"o\", label=\"RMSE\")\n",
    "      ax1.plot(years, metrics_df[\"mae\"], marker=\"o\", label=\"MAE\")\n",
    "      ax1.set_title(f\"RMSE & MAE ({target_horizon}-Day Horizon)\")\n",
    "      ax1.set_xlabel(\"Year\")\n",
    "      ax1.set_ylabel(\"Error Value\")\n",
    "      ax1.grid(True, alpha=0.3)\n",
    "      ax1.legend()\n",
    "      \n",
    "      filename1 = f\"metrics_target_{target_horizon}_RMSE_MAE.png\"\n",
    "      fig1.tight_layout()\n",
    "      fig1.savefig(os.path.join(folder_name, \"RMSE_MAE.png\"), dpi=150)\n",
    "      plt.close(fig1)\n",
    "\n",
    "\n",
    "      # --- Plot 2: Directional Accuracy ---\n",
    "      \n",
    "      fig2, ax2 = plt.subplots(figsize=(8, 4))\n",
    "      ax2.plot(years, metrics_df[\"directional_accuracy (%)\"], marker=\"s\", linestyle=\"--\")\n",
    "      ax2.set_ylim(0, 100)\n",
    "      ax2.set_title(f\"Directional Accuracy ({target_horizon}-Day Horizon)\")\n",
    "      ax2.set_xlabel(\"Year\")\n",
    "      ax2.set_ylabel(\"Directional Accuracy (%)\")\n",
    "      ax2.grid(True, alpha=0.3)\n",
    "\n",
    "      filename2 = f\"metrics_target_{target_horizon}_Directional_Accuracy.png\"\n",
    "      fig2.tight_layout()\n",
    "      fig2.savefig(os.path.join(folder_name, \"Directional_Accuracy.png\"), dpi=150)\n",
    "      plt.close(fig2)\n",
    "\n",
    "\n",
    "      # --- Plot 3: Spearman Correlation ---\n",
    "      fig3, ax3 = plt.subplots(figsize=(8, 4))\n",
    "      ax3.plot(years, metrics_df[\"spearman\"], marker=\"s\", linestyle=\":\")\n",
    "      ax3.set_title(f\"Spearman Correlation ({target_horizon}-Day Horizon)\")\n",
    "      ax3.set_xlabel(\"Year\")\n",
    "      ax3.set_ylabel(\"Spearman\")\n",
    "      ax3.grid(True, alpha=0.3)\n",
    "\n",
    "      filename3 = f\"metrics_target_{target_horizon}_Spearman.png\"\n",
    "      fig3.tight_layout()\n",
    "      fig3.savefig(os.path.join(folder_name, \"Spearman.png\"), dpi=150)\n",
    "      plt.close(fig3)\n",
    "      \n",
    "      \n",
    "      # save the df for good measure\n",
    "      df_file = f\"metrics_target_{target_horizon}.csv\"\n",
    "      # optional table save \n",
    "      if save_table:\n",
    "        metrics_df.to_csv(os.path.join(folder_name, \"metrics.csv\"), index=False)\n",
    "      \n",
    "\n",
    "      print(\"\\nSaved plots:\")\n",
    "      print(f\" - {filename1}\")\n",
    "      print(f\" - {filename2}\")\n",
    "      print(f\" - {filename3}\")\n",
    "      print(f\" - {df_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9acadc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_target_metrics(base_path=\".\"):\n",
    "    \"\"\"\n",
    "    Collects metrics.csv files from folders named result_target_[TARGET]\n",
    "    and returns a combined dataframe sorted by TARGET value.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_results = []\n",
    "    print(\"Folders found:\", os.listdir(base_path))\n",
    "    \n",
    "    # Regex to detect folders like result_target_5, result_target_10, etc.\n",
    "    folder_pattern = re.compile(r\"target_(\\d+)\")\n",
    "    for folder_name in os.listdir(base_path):\n",
    "        \n",
    "        match = folder_pattern.match(folder_name)\n",
    "        if match:\n",
    "            target_value = int(match.group(1))\n",
    "            metrics_file = os.path.join(base_path, folder_name, \"metrics.csv\")\n",
    "            # print(f\"Checking: {metrics_file}\")\n",
    "\n",
    "            if os.path.exists(metrics_file):\n",
    "                df = pd.read_csv(metrics_file)\n",
    "                df[\"pred_target\"] = target_value\n",
    "                combined_results.append(df)\n",
    "            else:\n",
    "                print(f\"WARNING: {metrics_file} not found.\")\n",
    "\n",
    "    # Combine all collected data\n",
    "    if combined_results:\n",
    "        final_df = pd.concat(combined_results, ignore_index=True)\n",
    "        final_df = final_df.set_index([\"year\", \"pred_target\"]).sort_index()\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No metrics.csv files found.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51201bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(results):\n",
    "    \"\"\"\n",
    "    Generate yearly prediction performance metrics from model results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : list[pd.DataFrame]\n",
    "        A list of DataFrames where each contains:\n",
    "        ['date', 'prediction', 'actual']\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Metrics sorted by year containing:\n",
    "        RMSE, MAE, Directional Accuracy (%), and Spearman Rank Correlation.\n",
    "    \"\"\"\n",
    "\n",
    "    year_stats = []\n",
    "\n",
    "    for df_year in results:  # each entry is a year dataframe\n",
    "\n",
    "        year = df_year['date'].iloc[0].year  # extract year from first row's date\n",
    "\n",
    "        preds = df_year[\"prediction\"].values\n",
    "        actual = df_year[\"actual\"].values\n",
    "\n",
    "        # Metrics\n",
    "        rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "        mae = mean_absolute_error(actual, preds)\n",
    "        \n",
    "        # directional accuracy\n",
    "        direction_accuracy = (np.sign(preds) == np.sign(actual)).mean()\n",
    "\n",
    "        # Spearman correlation (ignore NaNs)\n",
    "        spearman_val, _ = spearmanr(actual, preds, nan_policy='omit')\n",
    "\n",
    "        year_stats.append({\n",
    "            \"year\": year,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"directional_accuracy (%)\": round(direction_accuracy * 100, 2),\n",
    "            \"spearman\": spearman_val\n",
    "        })\n",
    "\n",
    "    # Convert results into a table\n",
    "    return pd.DataFrame(year_stats).sort_values(\"year\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd844c96",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5355b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        last_hidden = hidden[-1]\n",
    "        return self.fc(last_hidden)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb37c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment(model, config, metrics_df, target_horizon):\n",
    "    \n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(\"models\", f\"experiment_target_{target_horizon}.pth\")\n",
    "    torch.save({\n",
    "        \"model_config\": config,\n",
    "        \"weights\": model.state_dict(),\n",
    "        \"metrics\": metrics_df.to_dict(),\n",
    "        \"target_horizon\": target_horizon\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    # Save config separately as JSON\n",
    "    config_path = os.path.join(\"models\", f\"config_target_{target_horizon}.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    print(f\"\\nSaved model + config to models/:\")\n",
    "    print(f\" - {checkpoint_path}\")\n",
    "    print(f\" - {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d27202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_target(target, input_dim, hidden_dim, num_layers, dropout):\n",
    "    model_path = f\"model_target_{target}.pt\"\n",
    "    \n",
    "    model = LSTMPredictor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd53d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model: LSTMPredictor, config, metrics, target):\n",
    "    checkpoint = {\n",
    "        \"model_config\": config,\n",
    "        \"weights\": model.state_dict(),\n",
    "        \"metrics\": metrics.to_dict(),\n",
    "        \"target_horizon\": target\n",
    "    }\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    path = f\"models/experiment_target_{target}.pth\"\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "    print(f\"Saved experiment: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b00fb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model = LSTMPredictor(**checkpoint[\"model_config\"])\n",
    "    model.load_state_dict(checkpoint[\"weights\"])\n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc95dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_config(config, target):\n",
    "    \"\"\"\n",
    "    Stores the hyperparameter configuration of the trained LSTM\n",
    "    \"\"\"\n",
    "    folder = \"models\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    config_path = os.path.join(folder, f\"config_target_{target}.json\")\n",
    "\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    print(f\"[âœ“] Saved config to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "829e61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(target):\n",
    "    path = f\"models/config_target_{target}.json\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9a6a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seed(fixed=None):\n",
    "    \"\"\"Returns a deterministic seed if provided, otherwise generates a random one.\"\"\"\n",
    "    return fixed if fixed is not None else np.random.randint(1, 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f1824e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1545fc",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9c8e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_training_loop(df, target_col, feature_cols,\n",
    "    PRED_TARGET=15, \n",
    "    SEQ_LEN=10,\n",
    "    START_YEAR=2015,\n",
    "    END_YEAR=2025,\n",
    "    LEARNING_RATE=0.001,\n",
    "    HIDDEN_DIM=64,\n",
    "    NUM_LAYERS=2,\n",
    "    DROPOUT=0.2,\n",
    "    WEIGHT_SEED=42,\n",
    "    BATCH_SIZE=32,\n",
    "    EPOCHS=10):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for year in range(START_YEAR, END_YEAR):\n",
    "        print(f\"\\n=== WALK {year} ===\")\n",
    "\n",
    "        # find actual first trading day of the year\n",
    "        test_start = df[df.index >= f\"{year}-01-01\"].index.min()\n",
    "        if pd.isna(test_start):\n",
    "            print(f\"[SKIP] No rows for year {year}\")\n",
    "            continue\n",
    "\n",
    "        # rolling expand training set, strict 1-year forecast window\n",
    "        train = df[df.index < test_start]\n",
    "        test  = df[(df.index >= test_start) & (df.index < f\"{year+1}-01-01\")]\n",
    "\n",
    "        if len(train) < SEQ_LEN:\n",
    "            print(f\"[WAIT] Not enough history ({len(train)} rows, need {SEQ_LEN}) â€” skipping.\")\n",
    "            continue\n",
    "\n",
    "        # scale on train only\n",
    "        scaler = StandardScaler()\n",
    "        scaled_train = scaler.fit_transform(train[feature_cols])\n",
    "        scaled_test  = scaler.transform(test[feature_cols])\n",
    "\n",
    "        train_scaled = train.copy()\n",
    "        test_scaled  = test.copy()\n",
    "        train_scaled[feature_cols] = scaled_train\n",
    "        test_scaled[feature_cols]  = scaled_test\n",
    "\n",
    "        # Make sequences\n",
    "        X_train, y_train = make_sequences(train_scaled, target_col, feature_cols, SEQ_LEN)\n",
    "        X_test,  y_test  = make_sequences(test_scaled,  target_col, feature_cols, SEQ_LEN)\n",
    "\n",
    "        # tensors\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "        X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "        # reproducibility\n",
    "        apply_seed(WEIGHT_SEED)\n",
    "\n",
    "        # fresh model\n",
    "        model = LSTMPredictor(\n",
    "            input_dim=len(feature_cols),\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            dropout=DROPOUT\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # optimizer\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        # train\n",
    "        for epoch in range(EPOCHS):\n",
    "            for xb, yb in loader:\n",
    "                opt.zero_grad()\n",
    "                loss = loss_fn(model(xb), yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        # predict\n",
    "        preds = model(X_test_t).detach().numpy().flatten()\n",
    "        pred_dates = test_scaled.index[SEQ_LEN:]\n",
    "\n",
    "        results.append(pd.DataFrame({\n",
    "            \"date\": pred_dates,\n",
    "            \"prediction\": preds,\n",
    "            \"actual\": y_test[-len(preds):]\n",
    "        }))\n",
    "\n",
    "    config = {\n",
    "        \"input_dim\": len(feature_cols),\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"num_layers\": NUM_LAYERS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"target_horizon\": PRED_TARGET,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"weight_init_seed\": WEIGHT_SEED,\n",
    "        \"batch_size\": BATCH_SIZE\n",
    "    }\n",
    "\n",
    "    return results, model, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c8aa9",
   "metadata": {},
   "source": [
    "#### Operation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d657a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INIT PARAMS\n",
    "    # Dataset/Time\n",
    "TICKER = \"AMZN\"\n",
    "START = \"2015-01-01\"\n",
    "END = \"2025-01-01\"\n",
    "Y_START = 2015\n",
    "Y_END = 2022\n",
    "WARMUP = 70\n",
    "TARGET = 15\n",
    "    # Model\n",
    "SEED = make_seed(42)\n",
    "LOOKBACK = 10\n",
    "DIM = 64\n",
    "LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f0742e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['META']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[ERROR] No data returned for ticker 'META'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Load full data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mload_price_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTICKER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTART\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleadup_days\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTARGET\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2. Create walk-forward boundary\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# leave last two years for final testing untouched during training.\u001b[39;00m\n\u001b[32m     13\u001b[39m train_df = df[df.index < \u001b[33m\"\u001b[39m\u001b[33m2023-01-01\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mload_price_data\u001b[39m\u001b[34m(ticker, start_date, end_date, pred_target, leadup_days, interval, sma_vals)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# ---- Validate ----\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERROR] No data returned for ticker \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# ---- Fix timezone inconsistencies safely ----\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: [ERROR] No data returned for ticker 'META'."
     ]
    }
   ],
   "source": [
    "# 1. Load full data\n",
    "df = load_price_data(\n",
    "    ticker=TICKER,\n",
    "    start_date=START,\n",
    "    end_date=END,\n",
    "    leadup_days=WARMUP,\n",
    "    pred_target=TARGET\n",
    ")\n",
    "\n",
    "# 2. Create walk-forward boundary\n",
    "\n",
    "# leave last two years for final testing untouched during training.\n",
    "train_df = df[df.index < \"2023-01-01\"]\n",
    "test_df  = df[df.index >= \"2023-01-01\"]\n",
    "\n",
    "\n",
    "# 3. Train the model using WalkForward testing split\n",
    "# use all columns as features except target column\n",
    "feature_cols = [col for col in df.columns if col not in [f\"{TARGET}_day_target\"]]\n",
    "target_col = df[f\"{TARGET}_day_target\"]\n",
    "\n",
    "results, model, config = walk_forward_training_loop(\n",
    "    train_df, target_col, \n",
    "    feature_cols,\n",
    "    PRED_TARGET=TARGET,\n",
    "    SEQ_LEN=LOOKBACK, \n",
    "    START_YEAR=Y_START, \n",
    "    END_YEAR=Y_END, \n",
    "    LEARNING_RATE= LEARNING_RATE, \n",
    "    HIDDEN_DIM= DIM, \n",
    "    NUM_LAYERS= LAYERS, \n",
    "    DROPOUT= DROPOUT,\n",
    "    EPOCHS=EPOCHS,\n",
    "    WEIGHT_SEED=SEED,\n",
    "    BATCH_SIZE=BATCH_SIZE\n",
    "    \n",
    ")\n",
    "\n",
    "# 4. Convert results into a table\n",
    "metrics_df = generate_metrics(results=results)\n",
    "\n",
    "# 5. store results\n",
    "plot_and_save_metrics(metrics_df, TARGET)\n",
    "save_experiment(model, config, metrics_df, TARGET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b475003",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmetrics_df\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'metrics_df' is not defined"
     ]
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3abbde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders found: ['target_15', 'target_30']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>directional_accuracy (%)</th>\n",
       "      <th>spearman</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>pred_target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2015</th>\n",
       "      <th>15</th>\n",
       "      <td>0.083687</td>\n",
       "      <td>0.068770</td>\n",
       "      <td>70.25</td>\n",
       "      <td>-0.102168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.186073</td>\n",
       "      <td>0.156129</td>\n",
       "      <td>63.77</td>\n",
       "      <td>0.234522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2016</th>\n",
       "      <th>15</th>\n",
       "      <td>0.134896</td>\n",
       "      <td>0.113871</td>\n",
       "      <td>39.26</td>\n",
       "      <td>-0.032589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.568233</td>\n",
       "      <td>0.540696</td>\n",
       "      <td>34.47</td>\n",
       "      <td>0.057996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2017</th>\n",
       "      <th>15</th>\n",
       "      <td>0.101579</td>\n",
       "      <td>0.080987</td>\n",
       "      <td>34.85</td>\n",
       "      <td>0.119427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.247933</td>\n",
       "      <td>0.196842</td>\n",
       "      <td>39.80</td>\n",
       "      <td>-0.128993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2018</th>\n",
       "      <th>15</th>\n",
       "      <td>0.141982</td>\n",
       "      <td>0.118440</td>\n",
       "      <td>65.56</td>\n",
       "      <td>-0.259904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.208363</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>71.42</td>\n",
       "      <td>-0.049284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2019</th>\n",
       "      <th>15</th>\n",
       "      <td>0.076832</td>\n",
       "      <td>0.056928</td>\n",
       "      <td>73.55</td>\n",
       "      <td>0.192271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.219676</td>\n",
       "      <td>39.78</td>\n",
       "      <td>0.024605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2020</th>\n",
       "      <th>15</th>\n",
       "      <td>0.152371</td>\n",
       "      <td>0.127981</td>\n",
       "      <td>32.92</td>\n",
       "      <td>0.051678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.331040</td>\n",
       "      <td>0.308166</td>\n",
       "      <td>22.12</td>\n",
       "      <td>0.171632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2021</th>\n",
       "      <th>15</th>\n",
       "      <td>0.098242</td>\n",
       "      <td>0.069681</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.022344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.167258</td>\n",
       "      <td>0.134715</td>\n",
       "      <td>45.03</td>\n",
       "      <td>0.193358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      rmse       mae  directional_accuracy (%)  spearman\n",
       "year pred_target                                                        \n",
       "2015 15           0.083687  0.068770                     70.25 -0.102168\n",
       "     30           0.186073  0.156129                     63.77  0.234522\n",
       "2016 15           0.134896  0.113871                     39.26 -0.032589\n",
       "     30           0.568233  0.540696                     34.47  0.057996\n",
       "2017 15           0.101579  0.080987                     34.85  0.119427\n",
       "     30           0.247933  0.196842                     39.80 -0.128993\n",
       "2018 15           0.141982  0.118440                     65.56 -0.259904\n",
       "     30           0.208363  0.185902                     71.42 -0.049284\n",
       "2019 15           0.076832  0.056928                     73.55  0.192271\n",
       "     30           0.255556  0.219676                     39.78  0.024605\n",
       "2020 15           0.152371  0.127981                     32.92  0.051678\n",
       "     30           0.331040  0.308166                     22.12  0.171632\n",
       "2021 15           0.098242  0.069681                     50.00  0.022344\n",
       "     30           0.167258  0.134715                     45.03  0.193358"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_combined = collect_target_metrics(r\"C:\\Projects\\Workspaces\\NN\\results\")\n",
    "metrics_combined\n",
    "# metrics_combined.swaplevel(\"year\", \"pred_target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb7654",
   "metadata": {},
   "source": [
    "#### meta-experiment layer\n",
    "\n",
    "Goal: Finding the most suitable prediction horizon to use as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e62a3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_targets(df: pd.DataFrame, pred_targets: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds multiple prediction horizon target columns to the dataframe using\n",
    "    cumulative log returns.\n",
    "\n",
    "    Example:\n",
    "      pred_targets = [1, 5, 10, 15]\n",
    "      â†’ columns: [\"1_day_target\", \"5_day_target\", \"10_day_target\", \"15_day_target\"]\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Compute daily log returns once\n",
    "    daily_log_return = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "\n",
    "    for t in pred_targets:\n",
    "        df[f\"{t}_day_target\"] = (\n",
    "            daily_log_return.rolling(t).sum().shift(-t)\n",
    "        )\n",
    "\n",
    "    # Drop NaNs only once (after all targets are added)\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ab1cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_horizon_experiments(\n",
    "    df,\n",
    "    horizons,\n",
    "    base_seed=42,   # each horizon will mutate from this\n",
    "    seq_len=10,\n",
    "    start_year=2015,\n",
    "    end_year=2022,\n",
    "    learning_rate=0.001,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=10\n",
    "):\n",
    "\n",
    "    all_results = []  # stores summary for comparison\n",
    "\n",
    "    for i, horizon in enumerate(horizons):\n",
    "\n",
    "        \n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ðŸš€ Running experiment for horizon: {horizon} days\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # 1) Assign target column name for this horizon\n",
    "        target_col_name = f\"{horizon}_day_target\"\n",
    "\n",
    "        if target_col_name not in df.columns:\n",
    "            raise ValueError(f\"Missing column {target_col_name}. Did load_price_data generate these?\")\n",
    "\n",
    "        target_col = df[target_col_name]\n",
    "\n",
    "        # 2) Feature cols = everything except target\n",
    "        feature_cols = [c for c in df.columns if c != target_col_name]\n",
    "\n",
    "        # 3) Make reproducible but horizon-unique seed\n",
    "        seed = base_seed + (i * 137)  # guarantees different but traceable seeds\n",
    "\n",
    "        # 4) Walk-forward training\n",
    "        results, model, config = walk_forward_training_loop(\n",
    "            df[df.index < \"2023-01-01\"],   # train\n",
    "            target_col,\n",
    "            feature_cols,\n",
    "            WEIGHT_SEED=seed,\n",
    "            PRED_TARGET=horizon,\n",
    "            SEQ_LEN=seq_len,\n",
    "            START_YEAR=start_year,\n",
    "            END_YEAR=end_year,\n",
    "            LEARNING_RATE=learning_rate,\n",
    "            HIDDEN_DIM=hidden_dim,\n",
    "            NUM_LAYERS=num_layers,\n",
    "            DROPOUT=dropout,\n",
    "            BATCH_SIZE=batch_size,\n",
    "            EPOCHS=epochs\n",
    "        )\n",
    "\n",
    "        # 5) Compute metrics\n",
    "        metrics_df = generate_metrics(results)\n",
    "\n",
    "        # 6) Save model + metrics + plots\n",
    "        plot_and_save_metrics(metrics_df, horizon)\n",
    "        save_experiment(model, config, metrics_df, horizon)\n",
    "\n",
    "        # 7) Store summary row\n",
    "        all_results.append({\n",
    "            \"horizon\": horizon,\n",
    "            \"weight_seed\": seed,\n",
    "            \"avg_rmse\": metrics_df[\"rmse\"].mean(),\n",
    "            \"avg_mae\": metrics_df[\"mae\"].mean(),\n",
    "            \"avg_directional_accuracy\": metrics_df[\"directional_accuracy (%)\"].mean(),\n",
    "            \"avg_spearman\": metrics_df[\"spearman\"].mean()\n",
    "        })\n",
    "\n",
    "    # return final comparison table\n",
    "    return pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff372cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZONS = [1, 5, 10, 15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abfc98fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$META: possibly delisted; no timezone found\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mload_price_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTICKER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTART\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleadup_days\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWARMUP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTARGET\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mload_price_data\u001b[39m\u001b[34m(ticker, start_date, end_date, pred_target, leadup_days, interval, sma_vals)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Ensure columns are consistent\u001b[39;00m\n\u001b[32m     31\u001b[39m df = df.copy()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m df.index = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m()\n\u001b[32m     33\u001b[39m df.columns = df.columns.str.lower().str.replace(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# === Add Indicators ===\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# SMA\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Index' object has no attribute 'normalize'"
     ]
    }
   ],
   "source": [
    "df = load_price_data(\n",
    "    ticker=TICKER,\n",
    "    start_date=START,\n",
    "    end_date=END,\n",
    "    leadup_days=WARMUP,\n",
    "    pred_target=TARGET\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_horizon_df = generate_prediction_targets(df, HORIZONS)\n",
    "multi_horizon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cbbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ Running experiment for horizon: 1 days\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing column 1_day_target. Did load_price_data generate these?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m HORIZONS = [\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m15\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m25\u001b[39m, \u001b[32m30\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m summary_df = \u001b[43mrun_multi_horizon_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhorizons\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHORIZONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOOKBACK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY_START\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY_END\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAYERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDROPOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m summary_df.to_csv(\u001b[33m\"\u001b[39m\u001b[33mresults/horizon_comparison_summary.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(summary_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_multi_horizon_experiments\u001b[39m\u001b[34m(df, horizons, base_seed, seq_len, start_year, end_year, learning_rate, hidden_dim, num_layers, dropout, batch_size, epochs)\u001b[39m\n\u001b[32m     25\u001b[39m target_col_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhorizon\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_day_target\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_col_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Did load_price_data generate these?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m target_col = df[target_col_name]\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 2) Feature cols = everything except target\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Missing column 1_day_target. Did load_price_data generate these?"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "summary_df = run_multi_horizon_experiments(\n",
    "    df=train_df,\n",
    "    horizons=HORIZONS,\n",
    "    base_seed=SEED,\n",
    "    seq_len=LOOKBACK,\n",
    "    start_year=Y_START,\n",
    "    end_year=Y_END,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    hidden_dim=DIM,\n",
    "    num_layers=LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "summary_df.to_csv(\"results/horizon_comparison_summary.csv\", index=False)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbdd43",
   "metadata": {},
   "source": [
    "#### NEAT implementation of NeuroEvolutionary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4c774e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America/New_York\n"
     ]
    }
   ],
   "source": [
    "print(yf.Ticker(\"AMZN\").get_info().get(\"exchangeTimezoneName\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
