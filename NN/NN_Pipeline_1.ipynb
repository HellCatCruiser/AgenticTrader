{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a315296",
   "metadata": {},
   "source": [
    "1. Data Preparation\n",
    "- Load OHLCV data for all selected tickers (e.g., AMZN, META, AVGO, ETFs)\n",
    "- Normalize features per ticker (z-score or min-max scaling)\n",
    "- Add metadata: ticker_id, sector, day_of_week, macro regime (optional)\n",
    "- Create rolling windows for time series modeling (e.g., 10-day sequences)\n",
    "2. Feature Engineering\n",
    "- Technical indicators: RSI, MACD, Bollinger Bands, ATR\n",
    "- Candle features: range, body size, wick ratios\n",
    "- Volume features: OBV, VWAP, volume spikes\n",
    "- Lagged returns, volatility, momentum scores\n",
    "3. Labeling Strategy\n",
    "- Define swing trade targets:\n",
    "- Binary: Will price rise >x% in next n days?\n",
    "- Multi-class: Uptrend / Downtrend / Sideways\n",
    "- Regression: Expected return over next n days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b3713",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from datetime import datetime, timedelta\n",
    "from ta.momentum import RSIIndicator, StochRSIIndicator\n",
    "from ta.trend import MACD, SMAIndicator, EMAIndicator\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a640f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cu128\n",
      "12.8\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4080\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda) \n",
    "print(torch.cuda.is_available())   # True if a GPU is detected\n",
    "print(torch.cuda.device_count())   # Number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc264490",
   "metadata": {},
   "source": [
    "#### Datacollation\n",
    "\n",
    "Equities taken from realistic stock portfolio:\n",
    "\"AMZN\", \"META\", \"AVGO\", \"LLY\", \"ETN\", \"CYBR\", \"LIN\", \"WM\", \"SLNO\", \"CYTK\", \"XLV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4b969937",
   "metadata": {},
   "outputs": [],
   "source": [
    "leadup_days = 30\n",
    "start_date = (datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\") - timedelta(days=leadup_days)).strftime(\"%Y-%m-%d\")\n",
    "amzn = yf.Ticker(\"AMZN\")\n",
    "raw_data = amzn.history(start=start_date, end=\"2025-01-01\", interval=\"1d\", auto_adjust=True, actions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0d2b7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2537 entries, 2014-12-02 00:00:00-05:00 to 2024-12-31 00:00:00-05:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    2537 non-null   float64\n",
      " 1   High    2537 non-null   float64\n",
      " 2   Low     2537 non-null   float64\n",
      " 3   Close   2537 non-null   float64\n",
      " 4   Volume  2537 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 118.9 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2537.000000</td>\n",
       "      <td>2537.000000</td>\n",
       "      <td>2537.000000</td>\n",
       "      <td>2537.000000</td>\n",
       "      <td>2.537000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.546931</td>\n",
       "      <td>101.687477</td>\n",
       "      <td>99.302719</td>\n",
       "      <td>100.522593</td>\n",
       "      <td>7.616016e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.533373</td>\n",
       "      <td>55.155739</td>\n",
       "      <td>53.855255</td>\n",
       "      <td>54.506998</td>\n",
       "      <td>4.058225e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.314000</td>\n",
       "      <td>14.539500</td>\n",
       "      <td>14.262500</td>\n",
       "      <td>14.347500</td>\n",
       "      <td>1.500750e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>48.365501</td>\n",
       "      <td>47.702499</td>\n",
       "      <td>47.992001</td>\n",
       "      <td>5.077600e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>94.180000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>93.139999</td>\n",
       "      <td>94.230003</td>\n",
       "      <td>6.526200e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>153.692505</td>\n",
       "      <td>155.630005</td>\n",
       "      <td>151.550507</td>\n",
       "      <td>153.729996</td>\n",
       "      <td>9.000000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>232.389999</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>228.009995</td>\n",
       "      <td>232.929993</td>\n",
       "      <td>4.771220e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close        Volume\n",
       "count  2537.000000  2537.000000  2537.000000  2537.000000  2.537000e+03\n",
       "mean    100.546931   101.687477    99.302719   100.522593  7.616016e+07\n",
       "std      54.533373    55.155739    53.855255    54.506998  4.058225e+07\n",
       "min      14.314000    14.539500    14.262500    14.347500  1.500750e+07\n",
       "25%      48.000000    48.365501    47.702499    47.992001  5.077600e+07\n",
       "50%      94.180000    95.250000    93.139999    94.230003  6.526200e+07\n",
       "75%     153.692505   155.630005   151.550507   153.729996  9.000000e+07\n",
       "max     232.389999   233.000000   228.009995   232.929993  4.771220e+08"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.info()\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd8737",
   "metadata": {},
   "source": [
    "#### Initialize important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b6475af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indicators(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # General price dynamics\n",
    "    df[\"returns\"] = df[\"Close\"].pct_change()\n",
    "    df[\"logReturns\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "    df[\"Volatility\"] = df[\"returns\"].rolling(10).std()\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "    df['Body'] = abs(df['Close'] - df['Open'])\n",
    "    df['Wick'] = df['Range'] - df['Body']\n",
    "\n",
    "    # Momentum\n",
    "    df['RSI'] = RSIIndicator(df['Close'], window=14).rsi()\n",
    "    df['StochRSI'] = StochRSIIndicator(df['Close'], window=14).stochrsi()\n",
    "    \n",
    "    # Short-term moving averages (faster response)\n",
    "    df['SMA_10'] = SMAIndicator(df['Close'], window=10).sma_indicator()\n",
    "    df['EMA_10'] = EMAIndicator(df['Close'], window=10).ema_indicator()\n",
    "\n",
    "    # Medium-term moving averages (faster than previous 50)\n",
    "    df['SMA_20'] = SMAIndicator(df['Close'], window=20).sma_indicator()\n",
    "    df['EMA_20'] = EMAIndicator(df['Close'], window=20).ema_indicator()\n",
    "\n",
    "    # Optional: slightly faster “long-term” averages for crossovers\n",
    "    df['SMA_30'] = SMAIndicator(df['Close'], window=30).sma_indicator()\n",
    "    df['EMA_30'] = EMAIndicator(df['Close'], window=30).ema_indicator()\n",
    "\n",
    "    # Faster MACD for earlier crossovers\n",
    "    macd = MACD(df['Close'], window_slow=13, window_fast=6, window_sign=5)\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_Signal'] = macd.macd_signal()\n",
    "    \n",
    "    df['MACD_rel'] = df['MACD'] / df['Close']\n",
    "    df['MACD_Signal_rel'] = df['MACD_Signal'] / df['Close']\n",
    "    \n",
    "    # Volatility\n",
    "    bb = BollingerBands(df['Close'], window=20)\n",
    "    df['BB_High'] = bb.bollinger_hband()\n",
    "    df['BB_Low'] = bb.bollinger_lband()\n",
    "    df['ATR'] = AverageTrueRange(df['High'], df['Low'], df['Close'], window=14).average_true_range()\n",
    "    \n",
    "    # Volume\n",
    "    df['OBV'] = OnBalanceVolumeIndicator(df['Close'], df['Volume']).on_balance_volume()\n",
    "    df['OBV_prev'] = pd.to_numeric(df['OBV'].shift(1), errors='coerce').fillna(0).astype(int)\n",
    "    df['CMF'] = ChaikinMoneyFlowIndicator(df['High'], df['Low'], df['Close'], df['Volume'], window=20).chaikin_money_flow()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "36774e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" | Indicator(s)             | Rule                                                                            | Label |\n",
    "    | ------------------------ | ------------------------------------------------------------------------------- | ----- |\n",
    "    | MACD & MACD_Signal       | If `MACD > MACD_Signal` → **Buy**, if `MACD < MACD_Signal` → **Sell**           | ±1    |\n",
    "    | RSI                      | If `RSI < 30` → **Buy** (oversold), if `RSI > 70` → **Sell** (overbought)       | ±1    |\n",
    "    | StochRSI                 | If `StochRSI < 0.2` → **Buy**, if `StochRSI > 0.8` → **Sell**                   | ±1    |\n",
    "    | SMA / EMA                | If `Close > SMA_20` → **Buy**, if `Close < SMA_20` → **Sell**                   | ±1    |\n",
    "    | Bollinger Bands          | If `Close < BB_Low` → **Buy**, if `Close > BB_High` → **Sell**                  | ±1    |\n",
    "    | OBV (On-Balance Volume)  | If `OBV > OBV_prev` → **Buy**, if `OBV < OBV_prev` → **Sell**                   | ±1    |\n",
    "    | CMF (Chaikin Money Flow) | If `CMF > 0` → **Buy**, if `CMF < 0` → **Sell**                                 | ±1    |\n",
    "    | Volatility / ATR         | If `Volatility` or `ATR` is high, reduce conviction (set to `0` to avoid noise) | 0     |\n",
    "\"\"\"\n",
    "\n",
    "def signal_engine(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Initialize vote counter\n",
    "    votes = np.zeros(len(df), dtype=float)\n",
    "\n",
    "    # ---- Weighted voting system ----\n",
    "    # Stronger weights = more historically reliable / consistent indicator\n",
    "    w = {\n",
    "        # Core momentum/trend indicators\n",
    "        'MACD': 1.0,\n",
    "        'RSI': 0.8,\n",
    "        'StochRSI': 0.6,\n",
    "\n",
    "        # Short / medium / long-term trend indicators\n",
    "        'SMA_10': 0.9,           # Short-term trend\n",
    "        'SMA_20': 1.0,           # Medium-term trend\n",
    "        'SMA_30': 1.1,           # Long-term anchor\n",
    "        'SMA_cross': 1.0,        # 10 vs 20 crossover\n",
    "        'SMA_long_cross': 1.0,   # 20 vs 30 crossover\n",
    "\n",
    "        'EMA_10': 0.9,\n",
    "        'EMA_20': 1.0,\n",
    "        'EMA_30': 1.1,\n",
    "        'EMA_cross': 1.0,\n",
    "        'EMA_long_cross': 1.0,\n",
    "\n",
    "        # Volatility and confirmation indicators\n",
    "        'BB': 0.75,\n",
    "        'OBV': 0.9,\n",
    "        'CMF': 0.8,\n",
    "        'Body': 0.6,\n",
    "        'Wick': 0.8,\n",
    "        'Volatility': 0.4,\n",
    "    }\n",
    "\n",
    "    # --- Apply rules ---\n",
    "     # Shift indicators by 1 bar to avoid lookahead\n",
    "    df_shift = df.shift(1)\n",
    "\n",
    "    # --- Apply rules with shifted indicators ---\n",
    "    \n",
    "    # MACD crossover\n",
    "    votes += w['MACD'] * np.where(df_shift['MACD'] > df_shift['MACD_Signal'], 1, \n",
    "                                  np.where(df_shift['MACD'] < df_shift['MACD_Signal'], -1, 0))\n",
    "    \n",
    "    # RSI thresholds\n",
    "    votes += w['RSI'] * np.where(df_shift['RSI'] < 30, 1, \n",
    "                                 np.where(df_shift['RSI'] > 70, -1, 0))\n",
    "    \n",
    "    # StochRSI thresholds\n",
    "    votes += w['StochRSI'] * np.where(df_shift['StochRSI'] < 0.2, 1, \n",
    "                                      np.where(df_shift['StochRSI'] > 0.8, -1, 0))\n",
    "    \n",
    "    # RSI divergence (optional)\n",
    "    rsi_div = (df_shift['RSI'] > df_shift['RSI'].shift(1)) & (df_shift['Close'] < df_shift['Close'].shift(1))\n",
    "    votes += 0.6 * np.where(rsi_div, -1, 0)\n",
    "    \n",
    "    # --- Short-term and medium-term trend signals ---\n",
    "\n",
    "    # # SMA trend signals\n",
    "    # votes += w['SMA_10'] * np.where(df_shift['Close'] > df_shift['SMA_10'], 1,\n",
    "    #                                 np.where(df_shift['Close'] < df_shift['SMA_10'], -1, 0))\n",
    "    # votes += w['SMA_20'] * np.where(df_shift['Close'] > df_shift['SMA_20'], 1,\n",
    "    #                                 np.where(df_shift['Close'] < df_shift['SMA_20'], -1, 0))\n",
    "\n",
    "    # # SMA crossover (short vs medium)\n",
    "    # votes += w['SMA_cross'] * np.where(df_shift['SMA_10'] > df_shift['SMA_20'], 1,\n",
    "    #                                 np.where(df_shift['SMA_10'] < df_shift['SMA_20'], -1, 0))\n",
    "\n",
    "    # EMA trend signals\n",
    "    votes += w['EMA_10'] * np.where(df_shift['Close'] > df_shift['EMA_10'], 1,\n",
    "                                    np.where(df_shift['Close'] < df_shift['EMA_10'], -1, 0))\n",
    "    votes += w['EMA_20'] * np.where(df_shift['Close'] > df_shift['EMA_20'], 1,\n",
    "                                    np.where(df_shift['Close'] < df_shift['EMA_20'], -1, 0))\n",
    "\n",
    "    # EMA crossover (short vs medium)\n",
    "    votes += w['EMA_cross'] * np.where(df_shift['EMA_10'] > df_shift['EMA_20'], 1,\n",
    "                                    np.where(df_shift['EMA_10'] < df_shift['EMA_20'], -1, 0))\n",
    "\n",
    "    # Optional: long-term trend anchors\n",
    "    votes += w['SMA_30'] * np.where(df_shift['Close'] > df_shift['SMA_30'], 1,\n",
    "                                    np.where(df_shift['Close'] < df_shift['SMA_30'], -1, 0))\n",
    "    votes += w['EMA_30'] * np.where(df_shift['Close'] > df_shift['EMA_30'], 1,\n",
    "                                    np.where(df_shift['Close'] < df_shift['EMA_30'], -1, 0))\n",
    "    \n",
    "    # Long-term crossover (medium vs long)\n",
    "    votes += w['SMA_long_cross'] * np.where(df_shift['SMA_20'] > df_shift['SMA_30'], 1,\n",
    "                                            np.where(df_shift['SMA_20'] < df_shift['SMA_30'], -1, 0))\n",
    "    votes += w['EMA_long_cross'] * np.where(df_shift['EMA_20'] > df_shift['EMA_30'], 1,\n",
    "                                            np.where(df_shift['EMA_20'] < df_shift['EMA_30'], -1, 0))\n",
    "    \n",
    "    # Bollinger Bonds breakout\n",
    "    votes += w['BB'] * np.where(df_shift['Close'] < df_shift['BB_Low'], 1, \n",
    "                                np.where(df_shift['Close'] > df_shift['BB_High'], -1, 0))\n",
    "    \n",
    "    # OBV momentum\n",
    "    votes += w['OBV'] * np.where(df_shift['OBV'] > df_shift['OBV_prev'], 1, \n",
    "                                 np.where(df_shift['OBV'] < df_shift['OBV_prev'], -1, 0))\n",
    "    \n",
    "    # CNF accumulation/distribution\n",
    "    votes += w['CMF'] * np.where(df_shift['CMF'] > 0, 1, np.where(df_shift['CMF'] < 0, -1, 0))\n",
    "\n",
    "    # Candle body momentum\n",
    "    body_avg = df_shift['Body'].rolling(5, min_periods=1).mean()\n",
    "    votes += w['Body'] * np.where(df_shift['Body'] > body_avg, 1, np.where(df_shift['Body'] < body_avg, -1, 0))\n",
    "\n",
    "    # Wick exhaustion\n",
    "    votes += w['Wick'] * np.where(df_shift['Wick'] > df_shift['Range'] * 0.6, -1, \n",
    "                                  np.where(df_shift['Wick'] < df_shift['Range'] * 0.1, 1, 0))\n",
    "    \n",
    "    # Bearish engulfing candle\n",
    "    bear_engulf = (df_shift['Open'] < df_shift['Close']) & (df_shift['Close'] < df_shift['Open'].shift(1)) & (df_shift['Open'] > df_shift['Close'].shift(1))\n",
    "    votes += 0.8 * np.where(bear_engulf, -1, 0)\n",
    "\n",
    "\n",
    "    # Volatility contraction\n",
    "    vol_avg = df_shift['Volatility'].rolling(10, min_periods=1).mean()\n",
    "    votes += w['Volatility'] * np.where(df_shift['Volatility'] < vol_avg, 1, 0)\n",
    "\n",
    "    # --- Targeted volatility soft gate ---\n",
    "    atr_mean = df_shift['ATR'].rolling(50, min_periods=10).mean()\n",
    "    atr_std = df_shift['ATR'].rolling(50, min_periods=10).std()\n",
    "    high_atr = df_shift['ATR'] > (atr_mean + 2 * atr_std)\n",
    "\n",
    "    scale = np.ones(len(df))\n",
    "    scale[high_atr] = np.clip((atr_mean[high_atr] + 2 * atr_std[high_atr]) / df_shift['ATR'][high_atr], 0.3, 1.0)\n",
    "    votes *= scale\n",
    "\n",
    "    # --- Preliminary label ---\n",
    "    prelim_label = np.where(votes > 4, 1, np.where(votes < -4, -1, 0))\n",
    "\n",
    "    # --- Soft fallback logic for weak votes ---\n",
    "    fallback_zone = (prelim_label == 0) & (np.abs(votes) <= 2)\n",
    "    soft_vote_strength = 0.5\n",
    "    low_vol = df_shift['ATR'] < atr_mean\n",
    "    fallback_bear = (df_shift['EMA_10'] < df_shift['EMA_20']) & fallback_zone & low_vol\n",
    "    fallback_bull = (df_shift['EMA_10'] > df_shift['EMA_20']) & fallback_zone & low_vol\n",
    "\n",
    "    prelim_label[fallback_bull] = soft_vote_strength\n",
    "    prelim_label[fallback_bear] = -soft_vote_strength\n",
    "\n",
    "    # list the vote_totals\n",
    "    df['vote_total'] = votes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b48b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_target(df, horizon=1):\n",
    "    df[f'f_return_{horizon}'] = (\n",
    "        (1 + df['returns']).shift(-1).rolling(horizon).apply(lambda x: np.prod(x) - 1, raw=True)\n",
    "    )\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "aecb78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added Z-score normalization\n",
    "def normalize_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Exclude regression targets and bounded indicators\n",
    "    exclude_cols = [c for c in numeric_cols if c.startswith(\"f_return_\")] + [\n",
    "        \"RSI\", \"StochRSI\", \"CMF\"\n",
    "    ]\n",
    "    \n",
    "    cols_to_scale = [c for c in numeric_cols if c not in exclude_cols]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    return df, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9cc11402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data into sequences that an LSTM can consume.\n",
    "def build_sequences(df, target_col, window_size=30):\n",
    "    \"\"\"\n",
    "    Convert a feature DataFrame into sequences for LSTM training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with features and target column(s).\n",
    "    target_col : str\n",
    "        Name of the regression target column (e.g., 'f_return_1').\n",
    "    window_size : int\n",
    "        Number of past timesteps to include in each sequence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray\n",
    "        Feature sequences of shape (samples, timesteps, features).\n",
    "    y : np.ndarray\n",
    "        Target values aligned with each sequence.\n",
    "    feature_names : list\n",
    "        Names of the features used (for reference).\n",
    "    \"\"\"\n",
    "    # Features = all numeric columns except the target\n",
    "    feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c != target_col]\n",
    "    \n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(df) - window_size):\n",
    "        Xs.append(df[feature_cols].iloc[i:(i+window_size)].values)\n",
    "        ys.append(df[target_col].iloc[i+window_size])\n",
    "    \n",
    "    X = np.array(Xs)\n",
    "    y = np.array(ys)\n",
    "    \n",
    "    return X, y, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1b6cf520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_split_and_scale(X, y, n_splits=5, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Perform walk-forward splits and normalize each fold using only its training data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input features of shape (n_samples, window, n_features).\n",
    "    y : np.ndarray\n",
    "        Target values of shape (n_samples,).\n",
    "    n_splits : int\n",
    "        Number of folds (default=5).\n",
    "    train_ratio : float\n",
    "        Initial fraction of data to use for training (default=0.8).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    folds : list of tuples\n",
    "        Each tuple = (X_train_scaled, y_train, X_test_scaled, y_test, scaler)\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    n_samples = len(X)\n",
    "    split_size = int(n_samples * (1 - train_ratio) / n_splits)\n",
    "    start_train = int(n_samples * train_ratio)\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        # Define train/test indices\n",
    "        train_end = start_train + i * split_size\n",
    "        test_end = train_end + split_size\n",
    "        if test_end > n_samples:\n",
    "            break\n",
    "\n",
    "        X_train, y_train = X[:train_end], y[:train_end]\n",
    "        X_test, y_test = X[train_end:test_end], y[train_end:test_end]\n",
    "\n",
    "        # Fit scaler on training fold only\n",
    "        n_train, window, n_features = X_train.shape\n",
    "        X_train_flat = X_train.reshape(-1, n_features)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_flat)\n",
    "\n",
    "        # Transform train/test\n",
    "        X_train_scaled = scaler.transform(X_train_flat).reshape(n_train, window, n_features)\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape[0], window, n_features)\n",
    "\n",
    "        folds.append((X_train_scaled, y_train, X_test_scaled, y_test, scaler))\n",
    "\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea334ef",
   "metadata": {},
   "source": [
    "##### LSTM + train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "714e28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, units1=64, units2=32, dense_units=32, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # First LSTM layer (returns sequences)\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=units1, batch_first=True)\n",
    "        \n",
    "        # Second LSTM layer (returns final hidden state)\n",
    "        self.lstm2 = nn.LSTM(input_size=units1, hidden_size=units2, batch_first=True)\n",
    "        \n",
    "        # Dense + Dropout + Output\n",
    "        self.fc1 = nn.Linear(units2, dense_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(dense_units, 1)  # regression output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, features)\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, (h_n, _) = self.lstm2(out)\n",
    "        \n",
    "        # Take last hidden state from second LSTM\n",
    "        out = h_n[-1]  # shape: (batch, units2)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e8173dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(X_train, y_train, device, epochs=50, batch_size=32, patience=5, val_split=0.1):\n",
    "    \"\"\"\n",
    "    Train LSTM on given train set using PyTorch with validation and early stopping.\n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[2]\n",
    "    model = LSTMModel(input_size=n_features).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Train/validation split\n",
    "    n_val = int(len(X_train_tensor) * val_split)\n",
    "    X_val_tensor, y_val_tensor = X_train_tensor[-n_val:], y_train_tensor[-n_val:]\n",
    "    X_train_tensor, y_train_tensor = X_train_tensor[:-n_val], y_train_tensor[:-n_val]\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor).squeeze()\n",
    "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed4941",
   "metadata": {},
   "source": [
    "#### Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89045f0",
   "metadata": {},
   "source": [
    "#### Operation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Train (1970, 30, 30), Test (98, 30, 30)\n",
      "Early stopping at epoch 10\n",
      "Fold 0 Metrics: RMSE=0.0498, MAE=0.0378, DirAcc=93.88%\n",
      "Fold 1: Train (2068, 30, 30), Test (98, 30, 30)\n",
      "Early stopping at epoch 9\n",
      "Fold 1 Metrics: RMSE=0.0328, MAE=0.0268, DirAcc=81.63%\n",
      "Fold 2: Train (2166, 30, 30), Test (98, 30, 30)\n",
      "Early stopping at epoch 13\n",
      "Fold 2 Metrics: RMSE=0.0355, MAE=0.0260, DirAcc=86.73%\n",
      "Fold 3: Train (2264, 30, 30), Test (98, 30, 30)\n",
      "Early stopping at epoch 10\n",
      "Fold 3 Metrics: RMSE=0.0428, MAE=0.0376, DirAcc=68.37%\n",
      "Fold 4: Train (2362, 30, 30), Test (98, 30, 30)\n",
      "Early stopping at epoch 7\n",
      "Fold 4 Metrics: RMSE=0.0777, MAE=0.0644, DirAcc=67.35%\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.002  # 0.2% cutoff\n",
    "horizon = 15\n",
    "PATIENCE = 5\n",
    "BATCH_SIZE = 32\n",
    "WINDOW = 60\n",
    "EPOCHS = 20\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "# 1. Prepare data\n",
    "df = raw_data.copy()\n",
    "df = add_indicators(df)\n",
    "df = signal_engine(df)\n",
    "df = df.dropna()\n",
    "df = create_regression_target(df, horizon)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# 2. Build sequences\n",
    "X, y, features = build_sequences(df, target_col=f\"f_return_{horizon}\", window_size=30)\n",
    "folds = walk_forward_split_and_scale(X, y, n_splits=5)\n",
    "\n",
    "for i, (X_train, y_train, X_test, y_test, scaler) in enumerate(folds):\n",
    "    print(f\"Fold {i}: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "\n",
    "# 3. Train LSTM\n",
    "    model, history = train_lstm(\n",
    "        X_train, y_train,\n",
    "        device=device,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        patience=5,\n",
    "        val_split=0.1\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "# 4. Predict on test set\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "# 5. Metrics\n",
    "    rmse = np.sqrt(np.mean((preds - y_test) ** 2))\n",
    "    mae = np.mean(np.abs(preds - y_test))\n",
    "    dir_acc = np.mean((preds >= 0) == (y_test >= 0))\n",
    "\n",
    "    print(f\"Fold {i} Metrics: RMSE={rmse:.4f}, MAE={mae:.4f}, DirAcc={dir_acc:.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
